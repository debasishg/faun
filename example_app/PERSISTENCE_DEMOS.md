# SoA Persistence Demos

This directory contains demonstration programs showcasing different persistence backends for the SoA framework.

## Available Demos

### 1. Arrow In-Memory Persistence (`persistent_demo`)

**Purpose**: Demonstrates in-memory columnar persistence using Apache Arrow format.

**Use Cases**:
- Real-time applications requiring microsecond latency
- High-throughput OLTP workloads
- Session-based data processing
- Fast analytical queries on in-memory data

**Run**:
```bash
cargo run --bin persistent_demo
```

**Key Features**:
- Zero-copy SoA ↔ Arrow conversion
- Thread-safe in-memory storage
- Batch operations for efficiency
- Real-time memory statistics
- Perfect for development and testing

---

### 2. Parquet Disk Persistence (`parquet_demo`)

**Purpose**: Demonstrates durable disk-based persistence using Apache Parquet format.

**Use Cases**:
- Data that must survive application restarts
- Long-term data archival
- Integration with data science tools (Pandas, Spark, etc.)
- Space-efficient storage with compression
- Backup and recovery scenarios

**Run**:
```bash
cargo run --bin parquet_demo
```

**Key Features**:
- Durable disk storage (data survives restarts)
- Configurable compression (ZSTD, SNAPPY, GZIP, LZ4, BROTLI)
- Standard Parquet format (readable by Python, Spark, DuckDB)
- Efficient metadata operations (count without reading data)
- Async I/O for non-blocking operations
- Creates actual Parquet files you can inspect

**Output**:
- Creates `./parquet_demo_data/` directory
- Saves data in `data.parquet` file
- File can be read by Python, Spark, or any Parquet-compatible tool

---

## Comparing the Backends

| Feature | Arrow (In-Memory) | Parquet (Disk) |
|---------|------------------|----------------|
| **Speed** | Microseconds | Milliseconds (due to I/O) |
| **Durability** | Volatile (lost on restart) | Persistent (survives restart) |
| **Storage** | RAM only | Disk-based |
| **Compression** | None | Multiple algorithms |
| **File Format** | Arrow IPC | Parquet |
| **External Tools** | Arrow ecosystem | Universal (Spark, Pandas, etc.) |
| **Best For** | Real-time processing | Archival, analytics, backups |

## Reading Parquet Files with Python

After running the `parquet_demo`, you can read the generated Parquet file with Python:

```python
# Using Pandas
import pandas as pd
df = pd.read_parquet('./parquet_demo_data/data.parquet')
print(df)
print(df.describe())

# Using Polars (faster)
import polars as pl
df = pl.read_parquet('./parquet_demo_data/data.parquet')
print(df)

# Using PyArrow
import pyarrow.parquet as pq
table = pq.read_table('./parquet_demo_data/data.parquet')
print(table.to_pandas())
```

## Architecture

Both demos use the same domain model (`Order`) and SoA structure (`OrderSoA`), demonstrating how the persistence layer is completely abstracted:

```
Domain Model (Order)
        ↓
SoA Structure (OrderSoA) 
        ↓
ToArrow Trait (Arrow RecordBatch conversion)
        ↓
   ┌────────────────────┐
   │                    │
   ↓                    ↓
Arrow Persistence   Parquet Persistence
(In-Memory)        (Disk-Based)
```

The same `OrderSoA` structure can be persisted using either backend without changing the domain code!

## Next Steps

1. **Try both demos** to see the difference between in-memory and disk persistence
2. **Inspect the Parquet file** generated by `parquet_demo` using Python or DuckDB
3. **Compare performance** - Arrow is faster, Parquet provides durability
4. **Experiment with compression** - Try different algorithms in the Parquet demo
5. **Integration** - Use the Parquet file in your data science workflows

## Future Enhancements

- **DuckDB Integration** - SQL queries on SoA data (Phase 3)
- **Hybrid Storage** - Hot data in Arrow, cold data in Parquet
- **Partitioning** - Large dataset partitioning for Parquet
- **Cloud Storage** - S3/Azure Blob integration
- **Streaming** - Real-time ingestion with periodic Parquet snapshots
